{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maykon1313/Trabalho1_IA/blob/main/Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94e72b7",
      "metadata": {
        "id": "a94e72b7"
      },
      "source": [
        "# Notebook para o Trabalho 1 de IA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0) Preparações Iniciais\n",
        "\n"
      ],
      "metadata": {
        "id": "qYDu-zopg1Lm"
      },
      "id": "qYDu-zopg1Lm"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHsTDk7JfM-l",
        "outputId": "76d724a9-89e1-4f09-ad89-5f4bdced2348"
      },
      "id": "cHsTDk7JfM-l",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c08557f",
        "outputId": "a443e246-592f-4677-a9f3-5f5674ab9f1c"
      },
      "source": [
        "# Change the current working directory to your specific folder\n",
        "%cd /content/drive/MyDrive/1-Faculdade/4-Semestre/IA/"
      ],
      "id": "7c08557f",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/1-Faculdade/4-Semestre/IA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "62cf8923",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cf8923",
        "outputId": "d38cc32a-aae6-492d-d3c5-ba6999b16cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.14.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 5)) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 5)) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 5)) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 5)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 5)) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Instalação das dependências\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a434fa5",
      "metadata": {
        "id": "1a434fa5"
      },
      "source": [
        "### 1) Scraper (célula separada)\n",
        "\n",
        "O código coleta feitiços das páginas de `dndbeyond` e salva em `data/raw/feiticos.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf2e3ac",
      "metadata": {
        "id": "1cf2e3ac"
      },
      "source": [
        "### 2) Verificar CSV (célula separada)\n",
        "\n",
        "Faz uma verificação simples para garantir que o CSV tem 3 colunas por linha."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870a8948",
      "metadata": {
        "id": "870a8948"
      },
      "source": [
        "### 3) Separar dataset (célula separada)\n",
        "\n",
        "Divide `data/raw/feiticos.csv` em treino/val/test e salva em `data/separated/`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4491815e",
      "metadata": {
        "id": "4491815e"
      },
      "source": [
        "### 4) Embeddings (células separadas)\n",
        "\n",
        "Gera embeddings a partir dos CSVs separados e salva em `data/feiticos_embeddings.npz`. Também adicionamos uma célula para visualizar shapes e um exemplo de embedding de um texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3849a48b",
      "metadata": {
        "id": "3849a48b"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_and_save_embeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', data_dir='data/separated', out_path='data/feiticos_embeddings.npz'):\n",
        "    train_df = pd.read_csv(os.path.join(data_dir, 'feiticos_train.csv'))\n",
        "    val_df = pd.read_csv(os.path.join(data_dir, 'feiticos_val.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, 'feiticos_test.csv'))\n",
        "\n",
        "    train_texts = train_df['descricao'].tolist()\n",
        "    validation_texts = val_df['descricao'].tolist()\n",
        "    test_texts = test_df['descricao'].tolist()\n",
        "\n",
        "    train_label = train_df['escola'].tolist()\n",
        "    validation_label = val_df['escola'].tolist()\n",
        "    test_label = test_df['escola'].tolist()\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    train_embeddings = model.encode(train_texts, convert_to_numpy=True)\n",
        "    validation_embeddings = model.encode(validation_texts, convert_to_numpy=True)\n",
        "    test_embeddings = model.encode(test_texts, convert_to_numpy=True)\n",
        "\n",
        "    print(f\"\\nNúmero de textos de treino: {len(train_texts)}\")\n",
        "    print(f\"Formato dos embeddings de treino: {train_embeddings.shape}\")\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        train_embeddings=train_embeddings,\n",
        "        validation_embeddings=validation_embeddings,\n",
        "        test_embeddings=test_embeddings,\n",
        "        train_label=train_label,\n",
        "        validation_label=validation_label,\n",
        "        test_label=test_label\n",
        "    )\n",
        "\n",
        "\n",
        "def load_embeddings(path='data/feiticos_embeddings.npz'):\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    print({k: data[k].shape if hasattr(data[k], 'shape') else len(data[k]) for k in data.files})\n",
        "    return data\n",
        "\n",
        "# exemplo rápido de embedding de texto\n",
        "\n",
        "def example_text_embedding(text, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    emb = model.encode([text], convert_to_numpy=True)\n",
        "    print('Shape do embedding:', emb.shape)\n",
        "    return emb[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19163857",
      "metadata": {
        "id": "19163857"
      },
      "source": [
        "### 5) KNN (células separadas)\n",
        "\n",
        "Células para carregar embeddings, treinar KNN e um exemplo de predição (vetor -> label -> texto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f7bd13d2",
      "metadata": {
        "id": "f7bd13d2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def knn_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
        "    data = np.load(embeddings_path, allow_pickle=True)\n",
        "    train_embeddings = data['train_embeddings']\n",
        "    validation_embeddings = data['validation_embeddings']\n",
        "    test_embeddings = data['test_embeddings']\n",
        "\n",
        "    train_label = data['train_label']\n",
        "    validation_label = data['validation_label']\n",
        "    test_label = data['test_label']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
        "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "    print('Shapes (train, val, test):', train_embeddings_scaled.shape, validation_embeddings_scaled.shape, test_embeddings_scaled.shape)\n",
        "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
        "\n",
        "\n",
        "def knn_train(k_values=list(range(1,11)), metrics=['cosine','euclidean']):\n",
        "    best_k = 1\n",
        "    best_model = None\n",
        "    best_metric = metrics[0]\n",
        "    best_accuracy = 0\n",
        "\n",
        "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = knn_load()\n",
        "\n",
        "    for k in k_values:\n",
        "        for metric in metrics:\n",
        "            try:\n",
        "                knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
        "                knn.fit(train_embeddings_scaled, train_label)\n",
        "                val_predictions = knn.predict(validation_embeddings_scaled)\n",
        "                val_accuracy = accuracy_score(validation_label, val_predictions)\n",
        "                print(f\"K: {k}, Métrica: {metric}, Acurácia: {val_accuracy:.4f}\")\n",
        "                if val_accuracy > best_accuracy:\n",
        "                    best_k = k\n",
        "                    best_model = knn\n",
        "                    best_metric = metric\n",
        "                    best_accuracy = val_accuracy\n",
        "            except Exception as e:\n",
        "                print(f\"Erro com k={k}, metric={metric}: {e}\")\n",
        "\n",
        "    print('Melhor K encontrado:', best_k, best_metric, best_accuracy)\n",
        "    return best_k, best_model, best_metric, scaler\n",
        "\n",
        "# Exemplo de predição (usa model + scaler)\n",
        "def knn_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model_text = SentenceTransformer(model_name)\n",
        "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
        "    emb_scaled = scaler.transform(emb)\n",
        "    pred = model.predict(emb_scaled)\n",
        "    print('Predição label:', pred[0])\n",
        "    return pred[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d07f93",
      "metadata": {
        "id": "89d07f93"
      },
      "source": [
        "### 6) KNN + PCA + SMOTE (células separadas)\n",
        "\n",
        "Load, treino e exemplo de predição (retorna label codificado -> decodifique com LabelEncoder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fef5440d",
      "metadata": {
        "id": "fef5440d"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "def knn_pca_smote_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
        "    data = np.load(embeddings_path, allow_pickle=True)\n",
        "    train_embeddings = data['train_embeddings']\n",
        "    validation_embeddings = data['validation_embeddings']\n",
        "    test_embeddings = data['test_embeddings']\n",
        "\n",
        "    train_label = data['train_label']\n",
        "    validation_label = data['validation_label']\n",
        "    test_label = data['test_label']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_label_encoded = label_encoder.fit_transform(train_label)\n",
        "    validation_label_encoded = label_encoder.transform(validation_label)\n",
        "    test_label_encoded = label_encoder.transform(test_label)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
        "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_embeddings_balanced, train_label_balanced = smote.fit_resample(train_embeddings_scaled, train_label_encoded)\n",
        "\n",
        "    pca = PCA(n_components=0.95, random_state=42)\n",
        "    train_embeddings_pca = pca.fit_transform(train_embeddings_balanced)\n",
        "    validation_embeddings_pca = pca.transform(validation_embeddings_scaled)\n",
        "    test_embeddings_pca = pca.transform(test_embeddings_scaled)\n",
        "\n",
        "    print('After SMOTE and PCA shapes:', train_embeddings_pca.shape, validation_embeddings_pca.shape, test_embeddings_pca.shape)\n",
        "    return train_label_balanced, validation_label_encoded, test_label_encoded, train_embeddings_pca, validation_embeddings_pca, test_embeddings_pca, scaler, pca, label_encoder\n",
        "\n",
        "\n",
        "def knn_pca_smote_train(k_values=list(range(1,11)), metrics=['cosine','euclidean']):\n",
        "    best_k = 1\n",
        "    best_model = None\n",
        "    best_metric = metrics[0]\n",
        "    best_accuracy = 0\n",
        "\n",
        "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler, pca, label_encoder = knn_pca_smote_load()\n",
        "\n",
        "    for k in k_values:\n",
        "        for metric in metrics:\n",
        "            try:\n",
        "                knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
        "                knn.fit(train_embeddings_scaled, train_label)\n",
        "                val_predictions = knn.predict(validation_embeddings_scaled)\n",
        "                val_accuracy = accuracy_score(validation_label, val_predictions)\n",
        "                print(f\"K: {k}, Métrica: {metric}, Acurácia: {val_accuracy:.4f}\")\n",
        "                if val_accuracy > best_accuracy:\n",
        "                    best_k = k\n",
        "                    best_model = knn\n",
        "                    best_metric = metric\n",
        "                    best_accuracy = val_accuracy\n",
        "            except Exception as e:\n",
        "                print(f\"Erro com k={k}, metric={metric}: {e}\")\n",
        "\n",
        "    print('Melhor K encontrado:', best_k, best_metric, best_accuracy)\n",
        "    return best_k, best_model, best_metric, scaler, pca, label_encoder\n",
        "\n",
        "# Exemplo de predição com decodificação\n",
        "\n",
        "def knn_pca_predict_example(model, scaler, pca, label_encoder, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model_text = SentenceTransformer(model_name)\n",
        "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
        "    emb_scaled = scaler.transform(emb)\n",
        "    emb_pca = pca.transform(emb_scaled)\n",
        "    pred = model.predict(emb_pca)\n",
        "    pred_decoded = label_encoder.inverse_transform(pred)\n",
        "    print('Predição:', pred_decoded[0])\n",
        "    return pred_decoded[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be4c3547",
      "metadata": {
        "id": "be4c3547"
      },
      "source": [
        "### 7) Logistic Regression (células separadas)\n",
        "\n",
        "Load, treino e exemplo de predição."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43c1a57",
      "metadata": {
        "id": "e43c1a57"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def logistic_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
        "    data = np.load(embeddings_path, allow_pickle=True)\n",
        "    train_embeddings = data['train_embeddings']\n",
        "    validation_embeddings = data['validation_embeddings']\n",
        "    test_embeddings = data['test_embeddings']\n",
        "\n",
        "    train_label = data['train_label']\n",
        "    validation_label = data['validation_label']\n",
        "    test_label = data['test_label']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
        "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "    print('Shapes loaded for logistic:', train_embeddings_scaled.shape)\n",
        "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
        "\n",
        "\n",
        "def logistic_train(c_values=[0.01,0.1,1,10,100]):\n",
        "    best_c = 0.01\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = logistic_load()\n",
        "\n",
        "    for c in c_values:\n",
        "        try:\n",
        "            model = LogisticRegression(C=c, max_iter=2000)\n",
        "            model.fit(train_embeddings_scaled, train_label)\n",
        "            val_accuracy = model.score(validation_embeddings_scaled, validation_label)\n",
        "            print(f\"C: {c}, Acurácia: {val_accuracy:.4f}\")\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_c = c\n",
        "                best_model = model\n",
        "                best_accuracy = val_accuracy\n",
        "        except Exception as e:\n",
        "            print(f\"Erro em C={c}: {e}\")\n",
        "\n",
        "    print('Melhor C encontrado:', best_c, best_accuracy)\n",
        "    return best_c, best_model, scaler\n",
        "\n",
        "\n",
        "def logistic_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model_text = SentenceTransformer(model_name)\n",
        "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
        "    emb_scaled = scaler.transform(emb)\n",
        "    pred = model.predict(emb_scaled)\n",
        "    print('Predição:', pred[0])\n",
        "    return pred[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba964f6d",
      "metadata": {
        "id": "ba964f6d"
      },
      "source": [
        "### 8) SVM (células separadas)\n",
        "\n",
        "Load, treino e exemplo de predição."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0ae430",
      "metadata": {
        "id": "3e0ae430"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def svm_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
        "    data = np.load(embeddings_path, allow_pickle=True)\n",
        "    train_embeddings = data['train_embeddings']\n",
        "    validation_embeddings = data['validation_embeddings']\n",
        "    test_embeddings = data['test_embeddings']\n",
        "\n",
        "    train_label = data['train_label']\n",
        "    validation_label = data['validation_label']\n",
        "    test_label = data['test_label']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
        "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "    print('Shapes loaded for svm:', train_embeddings_scaled.shape)\n",
        "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
        "\n",
        "\n",
        "def svm_train(c_values=[0.1,1,10,100], gamma_values=[0.001,0.01,0.1,1]):\n",
        "    best_c = 0.1\n",
        "    best_gamma = 0.001\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = svm_load()\n",
        "\n",
        "    for c in c_values:\n",
        "        for g in gamma_values:\n",
        "            try:\n",
        "                model = SVC(C=c, gamma=g, kernel='rbf')\n",
        "                model.fit(train_embeddings_scaled, train_label)\n",
        "                val_accuracy = model.score(validation_embeddings_scaled, validation_label)\n",
        "                print(f\"C: {c}, Gamma: {g}, Acurácia: {val_accuracy:.4f}\")\n",
        "                if val_accuracy > best_accuracy:\n",
        "                    best_c = c\n",
        "                    best_gamma = g\n",
        "                    best_model = model\n",
        "                    best_accuracy = val_accuracy\n",
        "            except Exception as e:\n",
        "                print(f\"Erro em C={c}, G={g}: {e}\")\n",
        "\n",
        "    print('Melhor C/G encontrado:', best_c, best_gamma, best_accuracy)\n",
        "    return best_c, best_gamma, best_model, scaler\n",
        "\n",
        "\n",
        "def svm_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model_text = SentenceTransformer(model_name)\n",
        "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
        "    emb_scaled = scaler.transform(emb)\n",
        "    pred = model.predict(emb_scaled)\n",
        "    print('Predição:', pred[0])\n",
        "    return pred[0]"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}