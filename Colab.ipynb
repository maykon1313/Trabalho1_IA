{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94e72b7",
   "metadata": {},
   "source": [
    "# Notebook consolidado dos scripts\n",
    "\n",
    "Este notebook reúne os scripts presentes em `src/` (coleta, verificação, separação, geração de embeddings e modelos: KNN, KNN+PCA+SMOTE, Regressão Logística e SVM).\n",
    "\n",
    "Organização:\n",
    "- Execute a célula 2 (instalação) antes de rodar qualquer função.\n",
    "- A célula 3 contém todo o código convertido para funções reutilizáveis — execute partes conforme necessário.\n",
    "- A célula 4 mostra instruções rápidas de uso.\n",
    "\n",
    "Observação: os caminhos são relativos ao repositório (por exemplo `data/raw/feiticos.csv`, `data/feiticos_embeddings.npz`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das dependências\n",
    "# Execute esta célula apenas uma vez em um ambiente novo (ex.: Colab ou venv)\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecd583",
   "metadata": {},
   "source": [
    "## Como usar\n",
    "\n",
    "1. (Opcional) Rodar `scraper_collect()` para baixar os feitiços. Atenção: é um web-scraper e pode demorar.\n",
    "2. Rodar `split_dataset()` para criar os arquivos em `data/separated/`.\n",
    "3. Rodar `generate_and_save_embeddings()` para gerar `data/feiticos_embeddings.npz`.\n",
    "4. Escolha um treino: `knn_train()`, `knn_pca_smote_train()`, `logistic_train()` ou `svm_train()`.\n",
    "\n",
    "Dicas:\n",
    "- Em Colab, execute a célula de instalação primeiro.\n",
    "- Se quiser apenas testar modelos rapidamente, você pode carregar o arquivo `data/feiticos_embeddings.npz` já presente no repositório.\n",
    "- Ajuste parâmetros como valores de K, C ou gamma nas chamadas de treino.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a434fa5",
   "metadata": {},
   "source": [
    "### 1) Scraper (célula separada)\n",
    "\n",
    "A função abaixo coleta páginas de `dndbeyond` e salva em `data/raw/feiticos.csv`. Execute somente se sabe o que está fazendo (muitas requisições podem ocorrer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab056f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def get_spell_info(spell_slug):\n",
    "    \"\"\"Busca descrição de um feitiço pela slug.\"\"\"\n",
    "    url = f\"https://www.dndbeyond.com/spells/{spell_slug}\"\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            description = \"\"\n",
    "            bonus_text = \"\"\n",
    "            desc_selectors = [\n",
    "                'div.more-info-content p',\n",
    "                'div.spell-description p',\n",
    "                'div.description p',\n",
    "                '.ddb-statblock-item-description p',\n",
    "                '.spell-content p'\n",
    "            ]\n",
    "            for selector in desc_selectors:\n",
    "                paragraphs = soup.select(selector)\n",
    "                if paragraphs:\n",
    "                    description = paragraphs[0].get_text(strip=True)\n",
    "                    if len(paragraphs) > 1:\n",
    "                        bonus_text = paragraphs[1].get_text(strip=True)\n",
    "                    break\n",
    "            return description, bonus_text\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def scraper_collect(start_page=1, end_page=46, output_path='data/raw/feiticos.csv', pause=0.5, resume=True):\n",
    "    existing_spells = set()\n",
    "    file_exists = os.path.exists(output_path)\n",
    "\n",
    "    if file_exists and resume:\n",
    "        print(\"Arquivo existente encontrado. Carregando dados já coletados...\")\n",
    "        with open(output_path, 'r', encoding='utf-8') as existing_file:\n",
    "            csv_reader = csv.reader(existing_file)\n",
    "            next(csv_reader, None)\n",
    "            for row in csv_reader:\n",
    "                if row:\n",
    "                    existing_spells.add(row[0])\n",
    "        print(f\"Encontrados {len(existing_spells)} feitiços já coletados.\")\n",
    "\n",
    "    mode = 'a' if file_exists else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8', newline='') as f:\n",
    "        if not file_exists:\n",
    "            f.write('nome,escola,descricao\\n')\n",
    "\n",
    "        for i in range(start_page, end_page+1):\n",
    "            URL = f\"https://www.dndbeyond.com/spells?page={i}\"\n",
    "            try:\n",
    "                page = requests.get(URL)\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "                spells = soup.find_all('div', class_='info', attrs={'data-type': 'spells'})\n",
    "\n",
    "                for spell in spells:\n",
    "                    spell_name = spell.get('data-slug', '')\n",
    "                    if spell_name in existing_spells:\n",
    "                        print(f\"Pulando {spell_name} (já coletado)\")\n",
    "                        continue\n",
    "\n",
    "                    school_element = spell.find('div', class_='school')\n",
    "                    spell_school = ''\n",
    "                    if school_element and hasattr(school_element, 'get'):\n",
    "                        classes = school_element.get('class', [])\n",
    "                        for cls in classes:\n",
    "                            if cls != 'school':\n",
    "                                spell_school = cls\n",
    "                                break\n",
    "\n",
    "                    description, bonus_text = get_spell_info(spell_name)\n",
    "\n",
    "                    f.write(f'\"{spell_name}\",\"{spell_school}\",\"{description} {bonus_text}\"\\n')\n",
    "                    f.flush()\n",
    "                    print(f\"Coletado: {spell_name}\")\n",
    "                    time.sleep(pause)\n",
    "\n",
    "                print(f\"Página {i} concluída\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro na página {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2e3ac",
   "metadata": {},
   "source": [
    "### 2) Verificar CSV (célula separada)\n",
    "\n",
    "Roda checagem simples para garantir que o CSV tem 3 colunas por linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def check_csv_fields(file_path='data/raw/feiticos.csv'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row_num, row in enumerate(reader, start=1):\n",
    "            if len(row) != 3:\n",
    "                print(f\"Row {row_num} has {len(row)} fields instead of 3: {row}\")\n",
    "            else:\n",
    "                print(f\"Row {row_num}: OK ({len(row)} fields)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a8948",
   "metadata": {},
   "source": [
    "### 3) Separar dataset (célula separada)\n",
    "\n",
    "Divide `data/raw/feiticos.csv` em treino/val/test e salva em `data/separated/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def split_dataset(input_csv='data/raw/feiticos.csv', out_dir='data/separated'):\n",
    "    raw = pd.read_csv(input_csv)\n",
    "    train_df, temp_ds = train_test_split(raw, test_size=0.30, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_ds, test_size=0.3333, random_state=42)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(out_dir, 'feiticos_train.csv'), index=False)\n",
    "    val_df.to_csv(os.path.join(out_dir, 'feiticos_val.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(out_dir, 'feiticos_test.csv'), index=False)\n",
    "\n",
    "    print(f\"Treino: {len(train_df)} linhas\")\n",
    "    print(f\"Val: {len(val_df)} linhas\")\n",
    "    print(f\"Teste: {len(test_df)} linhas\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Célula de preview do CSV (mostrar primeiras linhas)\n",
    "def preview_raw(n=5, file_path='data/raw/feiticos.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "    display(df.head(n))\n",
    "    return df.head(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491815e",
   "metadata": {},
   "source": [
    "### 4) Embeddings (células separadas)\n",
    "\n",
    "Gera embeddings a partir dos CSVs separados e salva em `data/feiticos_embeddings.npz`. Também adicionamos uma célula para visualizar shapes e um exemplo de embedding de um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_and_save_embeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', data_dir='data/separated', out_path='data/feiticos_embeddings.npz'):\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'feiticos_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join(data_dir, 'feiticos_val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'feiticos_test.csv'))\n",
    "\n",
    "    train_texts = train_df['descricao'].tolist()\n",
    "    validation_texts = val_df['descricao'].tolist()\n",
    "    test_texts = test_df['descricao'].tolist()\n",
    "\n",
    "    train_label = train_df['escola'].tolist()\n",
    "    validation_label = val_df['escola'].tolist()\n",
    "    test_label = test_df['escola'].tolist()\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    train_embeddings = model.encode(train_texts, convert_to_numpy=True)\n",
    "    validation_embeddings = model.encode(validation_texts, convert_to_numpy=True)\n",
    "    test_embeddings = model.encode(test_texts, convert_to_numpy=True)\n",
    "\n",
    "    print(f\"\\nNúmero de textos de treino: {len(train_texts)}\")\n",
    "    print(f\"Formato dos embeddings de treino: {train_embeddings.shape}\")\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        train_embeddings=train_embeddings,\n",
    "        validation_embeddings=validation_embeddings,\n",
    "        test_embeddings=test_embeddings,\n",
    "        train_label=train_label,\n",
    "        validation_label=validation_label,\n",
    "        test_label=test_label\n",
    "    )\n",
    "\n",
    "\n",
    "def load_embeddings(path='data/feiticos_embeddings.npz'):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    print({k: data[k].shape if hasattr(data[k], 'shape') else len(data[k]) for k in data.files})\n",
    "    return data\n",
    "\n",
    "# exemplo rápido de embedding de texto\n",
    "\n",
    "def example_text_embedding(text, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode([text], convert_to_numpy=True)\n",
    "    print('Shape do embedding:', emb.shape)\n",
    "    return emb[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19163857",
   "metadata": {},
   "source": [
    "### 5) KNN (células separadas)\n",
    "\n",
    "Células para carregar embeddings, treinar KNN e um exemplo de predição (vetor -> label -> texto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def knn_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    train_embeddings = data['train_embeddings']\n",
    "    validation_embeddings = data['validation_embeddings']\n",
    "    test_embeddings = data['test_embeddings']\n",
    "\n",
    "    train_label = data['train_label']\n",
    "    validation_label = data['validation_label']\n",
    "    test_label = data['test_label']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
    "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
    "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
    "\n",
    "    print('Shapes (train, val, test):', train_embeddings_scaled.shape, validation_embeddings_scaled.shape, test_embeddings_scaled.shape)\n",
    "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
    "\n",
    "\n",
    "def knn_train(k_values=list(range(1,11)), metrics=['cosine','euclidean']):\n",
    "    best_k = 1\n",
    "    best_model = None\n",
    "    best_metric = metrics[0]\n",
    "    best_accuracy = 0\n",
    "\n",
    "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = knn_load()\n",
    "\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                knn.fit(train_embeddings_scaled, train_label)\n",
    "                val_predictions = knn.predict(validation_embeddings_scaled)\n",
    "                val_accuracy = accuracy_score(validation_label, val_predictions)\n",
    "                print(f\"K: {k}, Métrica: {metric}, Acurácia: {val_accuracy:.4f}\")\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_k = k\n",
    "                    best_model = knn\n",
    "                    best_metric = metric\n",
    "                    best_accuracy = val_accuracy\n",
    "            except Exception as e:\n",
    "                print(f\"Erro com k={k}, metric={metric}: {e}\")\n",
    "\n",
    "    print('Melhor K encontrado:', best_k, best_metric, best_accuracy)\n",
    "    return best_k, best_model, best_metric, scaler\n",
    "\n",
    "# Exemplo de predição (usa model + scaler) \n",
    "def knn_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_text = SentenceTransformer(model_name)\n",
    "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
    "    emb_scaled = scaler.transform(emb)\n",
    "    pred = model.predict(emb_scaled)\n",
    "    print('Predição label:', pred[0])\n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d07f93",
   "metadata": {},
   "source": [
    "### 6) KNN + PCA + SMOTE (células separadas)\n",
    "\n",
    "Load, treino e exemplo de predição (retorna label codificado -> decodifique com LabelEncoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def knn_pca_smote_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    train_embeddings = data['train_embeddings']\n",
    "    validation_embeddings = data['validation_embeddings']\n",
    "    test_embeddings = data['test_embeddings']\n",
    "\n",
    "    train_label = data['train_label']\n",
    "    validation_label = data['validation_label']\n",
    "    test_label = data['test_label']\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_label_encoded = label_encoder.fit_transform(train_label)\n",
    "    validation_label_encoded = label_encoder.transform(validation_label)\n",
    "    test_label_encoded = label_encoder.transform(test_label)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
    "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
    "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    train_embeddings_balanced, train_label_balanced = smote.fit_resample(train_embeddings_scaled, train_label_encoded)\n",
    "\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    train_embeddings_pca = pca.fit_transform(train_embeddings_balanced)\n",
    "    validation_embeddings_pca = pca.transform(validation_embeddings_scaled)\n",
    "    test_embeddings_pca = pca.transform(test_embeddings_scaled)\n",
    "\n",
    "    print('After SMOTE and PCA shapes:', train_embeddings_pca.shape, validation_embeddings_pca.shape, test_embeddings_pca.shape)\n",
    "    return train_label_balanced, validation_label_encoded, test_label_encoded, train_embeddings_pca, validation_embeddings_pca, test_embeddings_pca, scaler, pca, label_encoder\n",
    "\n",
    "\n",
    "def knn_pca_smote_train(k_values=list(range(1,11)), metrics=['cosine','euclidean']):\n",
    "    best_k = 1\n",
    "    best_model = None\n",
    "    best_metric = metrics[0]\n",
    "    best_accuracy = 0\n",
    "\n",
    "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler, pca, label_encoder = knn_pca_smote_load()\n",
    "\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                knn.fit(train_embeddings_scaled, train_label)\n",
    "                val_predictions = knn.predict(validation_embeddings_scaled)\n",
    "                val_accuracy = accuracy_score(validation_label, val_predictions)\n",
    "                print(f\"K: {k}, Métrica: {metric}, Acurácia: {val_accuracy:.4f}\")\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_k = k\n",
    "                    best_model = knn\n",
    "                    best_metric = metric\n",
    "                    best_accuracy = val_accuracy\n",
    "            except Exception as e:\n",
    "                print(f\"Erro com k={k}, metric={metric}: {e}\")\n",
    "\n",
    "    print('Melhor K encontrado:', best_k, best_metric, best_accuracy)\n",
    "    return best_k, best_model, best_metric, scaler, pca, label_encoder\n",
    "\n",
    "# Exemplo de predição com decodificação\n",
    "\n",
    "def knn_pca_predict_example(model, scaler, pca, label_encoder, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_text = SentenceTransformer(model_name)\n",
    "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
    "    emb_scaled = scaler.transform(emb)\n",
    "    emb_pca = pca.transform(emb_scaled)\n",
    "    pred = model.predict(emb_pca)\n",
    "    pred_decoded = label_encoder.inverse_transform(pred)\n",
    "    print('Predição:', pred_decoded[0])\n",
    "    return pred_decoded[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c3547",
   "metadata": {},
   "source": [
    "### 7) Logistic Regression (células separadas)\n",
    "\n",
    "Load, treino e exemplo de predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def logistic_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    train_embeddings = data['train_embeddings']\n",
    "    validation_embeddings = data['validation_embeddings']\n",
    "    test_embeddings = data['test_embeddings']\n",
    "\n",
    "    train_label = data['train_label']\n",
    "    validation_label = data['validation_label']\n",
    "    test_label = data['test_label']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
    "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
    "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
    "\n",
    "    print('Shapes loaded for logistic:', train_embeddings_scaled.shape)\n",
    "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
    "\n",
    "\n",
    "def logistic_train(c_values=[0.01,0.1,1,10,100]):\n",
    "    best_c = 0.01\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = logistic_load()\n",
    "\n",
    "    for c in c_values:\n",
    "        try:\n",
    "            model = LogisticRegression(C=c, max_iter=2000)\n",
    "            model.fit(train_embeddings_scaled, train_label)\n",
    "            val_accuracy = model.score(validation_embeddings_scaled, validation_label)\n",
    "            print(f\"C: {c}, Acurácia: {val_accuracy:.4f}\")\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_c = c\n",
    "                best_model = model\n",
    "                best_accuracy = val_accuracy\n",
    "        except Exception as e:\n",
    "            print(f\"Erro em C={c}: {e}\")\n",
    "\n",
    "    print('Melhor C encontrado:', best_c, best_accuracy)\n",
    "    return best_c, best_model, scaler\n",
    "\n",
    "\n",
    "def logistic_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_text = SentenceTransformer(model_name)\n",
    "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
    "    emb_scaled = scaler.transform(emb)\n",
    "    pred = model.predict(emb_scaled)\n",
    "    print('Predição:', pred[0])\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba964f6d",
   "metadata": {},
   "source": [
    "### 8) SVM (células separadas)\n",
    "\n",
    "Load, treino e exemplo de predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ae430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def svm_load(embeddings_path='data/feiticos_embeddings.npz'):\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    train_embeddings = data['train_embeddings']\n",
    "    validation_embeddings = data['validation_embeddings']\n",
    "    test_embeddings = data['test_embeddings']\n",
    "\n",
    "    train_label = data['train_label']\n",
    "    validation_label = data['validation_label']\n",
    "    test_label = data['test_label']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
    "    validation_embeddings_scaled = scaler.transform(validation_embeddings)\n",
    "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
    "\n",
    "    print('Shapes loaded for svm:', train_embeddings_scaled.shape)\n",
    "    return train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler\n",
    "\n",
    "\n",
    "def svm_train(c_values=[0.1,1,10,100], gamma_values=[0.001,0.01,0.1,1]):\n",
    "    best_c = 0.1\n",
    "    best_gamma = 0.001\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    train_label, validation_label, test_label, train_embeddings_scaled, validation_embeddings_scaled, test_embeddings_scaled, scaler = svm_load()\n",
    "\n",
    "    for c in c_values:\n",
    "        for g in gamma_values:\n",
    "            try:\n",
    "                model = SVC(C=c, gamma=g, kernel='rbf')\n",
    "                model.fit(train_embeddings_scaled, train_label)\n",
    "                val_accuracy = model.score(validation_embeddings_scaled, validation_label)\n",
    "                print(f\"C: {c}, Gamma: {g}, Acurácia: {val_accuracy:.4f}\")\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_c = c\n",
    "                    best_gamma = g\n",
    "                    best_model = model\n",
    "                    best_accuracy = val_accuracy\n",
    "            except Exception as e:\n",
    "                print(f\"Erro em C={c}, G={g}: {e}\")\n",
    "\n",
    "    print('Melhor C/G encontrado:', best_c, best_gamma, best_accuracy)\n",
    "    return best_c, best_gamma, best_model, scaler\n",
    "\n",
    "\n",
    "def svm_predict_example(model, scaler, sentence, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_text = SentenceTransformer(model_name)\n",
    "    emb = model_text.encode([sentence], convert_to_numpy=True)\n",
    "    emb_scaled = scaler.transform(emb)\n",
    "    pred = model.predict(emb_scaled)\n",
    "    print('Predição:', pred[0])\n",
    "    return pred[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
